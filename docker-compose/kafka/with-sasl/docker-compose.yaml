version: "3.8"

# Kafka 安全部署版本 - 带SASL/SCRAM认证
# 适用于生产环境
# 使用 Apache Kafka 3.9.1 最新稳定版本

services:
  # Zookeeper 服务 - 支持SASL认证
  zookeeper:
    image: apache/kafka:3.9.1
    container_name: kafka-zookeeper-sasl
    hostname: zookeeper
    command: >
      sh -c "
      echo 'Creating JAAS config for Zookeeper...' &&
      mkdir -p /opt/kafka/config/sasl &&
      cat > /opt/kafka/config/sasl/zookeeper_jaas.conf << 'EOF'
      Server {
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username=\"admin\"
        password=\"admin-secret\";
      };
      EOF
      export KAFKA_OPTS=\"-Djava.security.auth.login.config=/opt/kafka/config/sasl/zookeeper_jaas.conf\" &&
      /opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties
      "
    environment:
      # Zookeeper SASL配置
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: SASL_PLAINTEXT://0.0.0.0:2181
      KAFKA_ADVERTISED_LISTENERS: SASL_PLAINTEXT://zookeeper:2181
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-256
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-256

      # 安全配置
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_CONTROLLER_PROTOCOL: SCRAM-SHA-256

      # JVM 安全配置
      KAFKA_OPTS: >-
        -Djava.security.auth.login.config=/opt/kafka/config/sasl/zookeeper_jaas.conf
        -Dzookeeper.sasl.client=true
        -Dzookeeper.sasl.clientconfig=Client

    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
      - zookeeper-sasl-config:/opt/kafka/config/sasl
    networks:
      - kafka-sasl-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
        reservations:
          memory: 512M
          cpus: "0.25"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Kafka Broker 服务 - 支持SASL认证
  kafka:
    image: apache/kafka:3.9.1
    container_name: kafka-broker-sasl
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    command: >
      sh -c "
      echo 'Creating JAAS config for Kafka...' &&
      mkdir -p /opt/kafka/config/sasl &&
      cat > /opt/kafka/config/sasl/kafka_server_jaas.conf << 'EOF'
      KafkaServer {
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username=\"admin\"
        password=\"admin-secret\";
      };
      Client {
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username=\"admin\"
        password=\"admin-secret\";
      };
      EOF

      echo 'Creating client SASL config...' &&
      cat > /opt/kafka/config/sasl/client.properties << 'EOF'
      security.protocol=SASL_PLAINTEXT
      sasl.mechanism=SCRAM-SHA-256
      sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";
      EOF

      echo 'Setting up SASL users...' &&
      export KAFKA_OPTS=\"-Djava.security.auth.login.config=/opt/kafka/config/sasl/kafka_server_jaas.conf\" &&

      # 启动 Kafka 并等待就绪后创建用户
      /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties &
      KAFKA_PID=\$! &&

      echo 'Waiting for Kafka to start...' &&
      sleep 30 &&

      echo 'Creating SCRAM users...' &&
      /opt/kafka/bin/kafka-configs.sh --zookeeper zookeeper:2181 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret]' --entity-type users --entity-name admin || true &&
      /opt/kafka/bin/kafka-configs.sh --zookeeper zookeeper:2181 --alter --add-config 'SCRAM-SHA-256=[password=user-secret]' --entity-type users --entity-name kafkauser || true &&
      /opt/kafka/bin/kafka-configs.sh --zookeeper zookeeper:2181 --alter --add-config 'SCRAM-SHA-256=[password=producer-secret]' --entity-type users --entity-name producer || true &&
      /opt/kafka/bin/kafka-configs.sh --zookeeper zookeeper:2181 --alter --add-config 'SCRAM-SHA-256=[password=consumer-secret]' --entity-type users --entity-name consumer || true &&

      echo 'SCRAM users created successfully' &&
      wait \$KAFKA_PID
      "
    environment:
      # Kafka 基础配置
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      # SASL 监听器配置
      KAFKA_LISTENERS: SASL_PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: SASL_PLAINTEXT://kafka:9092,SASL_PLAINTEXT://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SASL_PLAINTEXT:SASL_PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: SASL_PLAINTEXT

      # SASL 认证配置
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-256
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-256
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_PLAINTEXT

      # 日志和存储配置
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_NUM_NETWORK_THREADS: 3
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600

      # 分区和副本配置
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1

      # 日志保留配置
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000

      # 主题管理
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_DELETE_TOPIC_ENABLE: "true"

      # 组协调器配置
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 3000
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

      # JVM 配置
      KAFKA_HEAP_OPTS: "-Xmx2G -Xms2G"
      KAFKA_JVM_PERFORMANCE_OPTS: >-
        -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20
        -XX:InitiatingHeapOccupancyPercent=35
        -XX:+ExplicitGCInvokesConcurrent
        -Djava.awt.headless=true
        -Djava.security.auth.login.config=/opt/kafka/config/sasl/kafka_server_jaas.conf

      # 安全和监控配置
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"

      # ACL 配置
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_SUPER_USERS: User:admin
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "false"

    ports:
      - "9092:9092"
      - "9093:9093" # 外部访问端口
    volumes:
      - kafka-data:/var/lib/kafka/data
      - kafka-logs:/opt/kafka/logs
      - kafka-sasl-config:/opt/kafka/config/sasl
    networks:
      - kafka-sasl-network
    restart: unless-stopped
    healthcheck:
      test: >
        bash -c "
        echo 'admin-secret' | /opt/kafka/bin/kafka-console-producer.sh 
        --bootstrap-server localhost:9092 
        --topic __health_check 
        --producer.config /opt/kafka/config/sasl/client.properties 
        --timeout-ms 5000 2>/dev/null || exit 1
        "
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "1.5"
        reservations:
          memory: 2G
          cpus: "1.0"
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Kafka UI 管理界面 - 支持SASL认证
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui-sasl
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: kafka-sasl-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

      # SASL 认证配置
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_PLAINTEXT
      KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: SCRAM-SHA-256
      KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: >-
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="admin"
        password="admin-secret";

      # UI 配置
      DYNAMIC_CONFIG_ENABLED: "true"
      AUTH_TYPE: DISABLED
      MANAGEMENT_HEALTH_LDAP_ENABLED: "false"

    ports:
      - "8080:8080"
    networks:
      - kafka-sasl-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:8080/actuator/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
        reservations:
          memory: 512M
          cpus: "0.25"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Schema Registry (可选) - 支持SASL认证
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: kafka-schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

      # SASL 认证配置
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: SASL_PLAINTEXT
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: SCRAM-SHA-256
      SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG: >-
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="admin"
        password="admin-secret";

      # Schema Registry 配置
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_DEBUG: "false"

    ports:
      - "8081:8081"
    networks:
      - kafka-sasl-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"
        reservations:
          memory: 512M
          cpus: "0.25"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Kafka Connect (可选) - 支持SASL认证
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    container_name: kafka-connect-sasl
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect-group

      # SASL 认证配置
      CONNECT_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONNECT_SASL_MECHANISM: SCRAM-SHA-256
      CONNECT_SASL_JAAS_CONFIG: >-
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="admin"
        password="admin-secret";

      # Producer 配置
      CONNECT_PRODUCER_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONNECT_PRODUCER_SASL_MECHANISM: SCRAM-SHA-256
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: >-
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="producer"
        password="producer-secret";

      # Consumer 配置
      CONNECT_CONSUMER_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONNECT_CONSUMER_SASL_MECHANISM: SCRAM-SHA-256
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: >-
        org.apache.kafka.common.security.scram.ScramLoginModule required
        username="consumer"
        password="consumer-secret";

      # Schema Registry 配置
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081

      # 内部主题配置
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1

      # 插件路径
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

    ports:
      - "8083:8083"
    networks:
      - kafka-sasl-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.0"
        reservations:
          memory: 1G
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "3"

# 数据卷定义
volumes:
  zookeeper-data:
    driver: local
    name: kafka-sasl-zookeeper-data
  zookeeper-logs:
    driver: local
    name: kafka-sasl-zookeeper-logs
  zookeeper-sasl-config:
    driver: local
    name: kafka-sasl-zookeeper-config
  kafka-data:
    driver: local
    name: kafka-sasl-kafka-data
  kafka-logs:
    driver: local
    name: kafka-sasl-kafka-logs
  kafka-sasl-config:
    driver: local
    name: kafka-sasl-kafka-config

# 网络定义
networks:
  kafka-sasl-network:
    driver: bridge
    name: kafka-sasl-network
    ipam:
      config:
        - subnet: 172.21.0.0/16
