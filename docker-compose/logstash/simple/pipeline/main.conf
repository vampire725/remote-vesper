# =================================================================
# Logstash 简单管道配置文件
# =================================================================
# 版本: 简化稳定版 - 兼容 Logstash 8.15.3
# 用途: 开发和测试环境，支持多种数据输入方式
# 特点: 去除了复杂的解析规则，专注于数据收集和基础处理
# =================================================================

# =================================================================
# 输入配置部分 - 从 Kafka 消费数据
# =================================================================
input {
  # Kafka 输入 - 从 Kafka 主题消费消息
  # 支持多个主题和分区，提供高吞吐量的数据摄取
  kafka {
    # Kafka 集群配置
    bootstrap_servers => ["kafka:9092"]         # Kafka broker 地址列表
    
    # 主题配置
    topics => ["app-logs", "events", "metrics"]     # 订阅的主题列表，可根据需要修改
    
    # 消费者配置
    group_id => "logstash-consumer-group"       # 消费者组ID，便于管理和监控
    client_id => "logstash-simple"              # 客户端ID标识
    
    # 序列化配置
    codec => json {                             # JSON 解码器
      charset => "UTF-8"                        # 字符编码
    }
    
    # 性能优化配置
    consumer_threads => 2                       # 消费者线程数，建议与pipeline workers一致
    fetch_min_bytes => 1                        # 最小获取字节数
    fetch_max_wait_ms => 500                    # 最大等待时间（毫秒）
    max_poll_records => 500                     # 单次拉取的最大记录数
    
    # 偏移量管理
    auto_offset_reset => "latest"               # 偏移量重置策略：latest(最新) 或 earliest(最早)
    enable_auto_commit => true                  # 自动提交偏移量
    auto_commit_interval_ms => 1000             # 自动提交间隔（毫秒）
    
    # 会话和心跳配置
    session_timeout_ms => 30000                 # 会话超时时间
    heartbeat_interval_ms => 3000               # 心跳间隔
    
    # 安全配置（可选，如果Kafka启用了安全认证）
    # security_protocol => "PLAINTEXT"          # 安全协议：PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL
    # sasl_mechanism => "PLAIN"                 # SASL机制
    # sasl_jaas_config => "..."                 # JAAS配置
    
    # 添加元数据字段
    decorate_events => true                     # 添加Kafka元数据到事件中
    
    # 类型标识
    type => "kafka-input"                       # 为来自Kafka的事件添加类型标识
  }
}

# =================================================================
# 过滤器配置部分 - 数据处理和增强
# =================================================================
filter {
  # 添加基本信息字段
  # 目的: 为每条日志记录添加处理时的元数据
  mutate {
    add_field => { 
      "logstash_host" => "%{host}"           # 记录处理日志的主机名
      "processed_by" => "logstash-simple"   # 标识处理管道名称
      "processed_at" => "%{@timestamp}"     # 记录处理时间戳
    }
  }
  
  # 数据清理 - 丢弃空消息
  # 目的: 避免处理和存储无用的空白日志条目
  if [message] == "" {
    drop { }              # 丢弃message字段为空的事件
  }
  
  # =================================================================
  # 时间戳解析和标准化
  # =================================================================
  
  # 解析自定义时间戳字段
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss'Z'", "yyyy-MM-dd HH:mm:ss" ]
      target => "@timestamp"              # 更新事件的主时间戳
      add_field => { "timestamp_parsed" => "true" }
    }
  }
  
  # 解析日志中的时间戳模式
  if [log_time] {
    date {
      match => [ "log_time", "dd/MMM/yyyy:HH:mm:ss Z" ]  # Apache/Nginx 格式
      target => "parsed_log_time"
    }
  }
  
  # =================================================================
  # 日志级别标准化
  # =================================================================
  
  # 标准化日志级别
  if [level] {
    mutate {
      lowercase => [ "level" ]            # 转换为小写
    }
    
    # 映射各种日志级别格式到标准格式
    if [level] in ["debug", "trace", "verbose"] {
      mutate { replace => { "level" => "debug" } }
    } else if [level] in ["info", "information", "notice"] {
      mutate { replace => { "level" => "info" } }
    } else if [level] in ["warn", "warning"] {
      mutate { replace => { "level" => "warn" } }
    } else if [level] in ["err", "error", "fatal", "critical"] {
      mutate { replace => { "level" => "error" } }
    }
    
    # 添加日志级别数值（用于排序和过滤）
    if [level] == "debug" {
      mutate { add_field => { "level_num" => 1 } }
    } else if [level] == "info" {
      mutate { add_field => { "level_num" => 2 } }
    } else if [level] == "warn" {
      mutate { add_field => { "level_num" => 3 } }
    } else if [level] == "error" {
      mutate { add_field => { "level_num" => 4 } }
    }
  }
  
  # =================================================================
  # 数值字段类型转换
  # =================================================================
  
  # 转换数值类型字段
  if [response_time] {
    mutate {
      convert => { "response_time" => "float" }
    }
  }
  
  if [status_code] {
    mutate {
      convert => { "status_code" => "integer" }
    }
  }
  
  if [bytes] {
    mutate {
      convert => { "bytes" => "integer" }
    }
  }
  
  if [level_num] {
    mutate {
      convert => { "level_num" => "integer" }
    }
  }
  
  # =================================================================
  # 嵌套 JSON 字段提取
  # =================================================================
  
  # 如果存在嵌套的 user 对象，提取其字段
  if [user] {
    if [user][id] {
      mutate { add_field => { "user_id" => "%{[user][id]}" } }
    }
    if [user][name] {
      mutate { add_field => { "user_name" => "%{[user][name]}" } }
    }
    if [user][email] {
      mutate { add_field => { "user_email" => "%{[user][email]}" } }
    }
  }
  
  # 如果存在 request 对象，提取请求信息
  if [request] {
    if [request][method] {
      mutate { add_field => { "http_method" => "%{[request][method]}" } }
    }
    if [request][url] {
      mutate { add_field => { "http_url" => "%{[request][url]}" } }
    }
    if [request][headers] and [request][headers][user-agent] {
      mutate { add_field => { "user_agent" => "%{[request][headers][user-agent]}" } }
    }
  }
  
  # =================================================================
  # IP 地址处理和地理位置解析
  # =================================================================
  
  # 提取 IP 地址字段
  if [client_ip] or [remote_addr] or [src_ip] {
    if [client_ip] {
      mutate { add_field => { "source_ip" => "%{client_ip}" } }
    } else if [remote_addr] {
      mutate { add_field => { "source_ip" => "%{remote_addr}" } }
    } else if [src_ip] {
      mutate { add_field => { "source_ip" => "%{src_ip}" } }
    }
    
    # 地理位置解析
    if [source_ip] and [source_ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.)/ {
      geoip {
        source => "source_ip"
        target => "geoip"
        add_field => { "has_geoip" => "true" }
      }
    }
  }
  
  # =================================================================
  # 用户代理解析
  # =================================================================
  
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }
  
  # =================================================================
  # HTTP 状态码分类
  # =================================================================
  
  if [status_code] {
    if [status_code] >= 200 and [status_code] < 300 {
      mutate { add_field => { "status_class" => "success" } }
    } else if [status_code] >= 300 and [status_code] < 400 {
      mutate { add_field => { "status_class" => "redirect" } }
    } else if [status_code] >= 400 and [status_code] < 500 {
      mutate { add_field => { "status_class" => "client_error" } }
    } else if [status_code] >= 500 {
      mutate { add_field => { "status_class" => "server_error" } }
    }
  }
  
  # =================================================================
  # 字段清理和重命名
  # =================================================================
  
  # 移除不需要的原始字段
  mutate {
    remove_field => [ "host" ]           # 移除原始host字段，保留logstash_host
  }
  
  # 重命名字段以符合ECS标准
  if [msg] {
    mutate { rename => { "msg" => "message" } }
  }
  
  if [svc] {
    mutate { rename => { "svc" => "service" } }
  }
  
  # =================================================================
  # 数据验证和质量检查
  # =================================================================
  
  # 添加数据质量标记
  mutate {
    add_field => { "data_quality_score" => 0 }
  }
  
  # 根据字段完整性评分
  if [service] {
    mutate { replace => { "data_quality_score" => "%{data_quality_score}1" } }
  }
  if [level] {
    mutate { replace => { "data_quality_score" => "%{data_quality_score}1" } }
  }
  if [user_id] {
    mutate { replace => { "data_quality_score" => "%{data_quality_score}1" } }
  }
  if [source_ip] {
    mutate { replace => { "data_quality_score" => "%{data_quality_score}1" } }
  }
  
  # 转换质量评分为数值
  mutate {
    convert => { "data_quality_score" => "integer" }
  }
}

# =================================================================
# 输出配置部分 - 定义数据去向
# =================================================================
output {
  # Elasticsearch 输出 - 将数据发送到 Elasticsearch 集群
  # 这是主要的数据存储和搜索后端
  elasticsearch {
    hosts => ["elasticsearch:9200"]              # ES集群地址，使用容器名称
    index => "logstash-%{+YYYY.MM.dd}"          # 索引名称，按日期分割便于管理
    
    # 连接设置
    timeout => 60                                # 连接超时时间（秒）
    
    # 禁用节点发现以避免IP地址缓存问题
    sniffing => false                            # 禁用自动节点发现
    
    # 连接池设置
    pool_max => 1000                             # 最大连接数
    pool_max_per_route => 100                    # 每个路由的最大连接数
  }
  
  # 控制台输出 - 实时显示处理的数据（开发调试用）
  # 用途: 开发调试时观察数据流和处理结果
  # 注意: 生产环境建议关闭以提高性能
  stdout {
    codec => json         # 以JSON格式在控制台显示
  }
} 