input {
  kafka {
    bootstrap_servers => "${KAFKA_HOSTS}"
    topics => ["${KAFKA_TOPIC}"]
    client_id => "logstash"
    group_id => "logstash"
    auto_offset_reset => "latest"
    consumer_threads => 2
    decorate_events => true
    sasl_mechanism => "PLAIN"
    security_protocol => "SASL_SSL"
    sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"${KAFKA_USERNAME}\" password=\"${KAFKA_PASSWORD}\";"
  }
}

filter {
  # 解析 JSON 格式的日志
  if [type] == "json" {
    json {
      source => "message"
    }
  }

  # 处理多行日志
  if [type] == "multiline" {
    multiline {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => true
      what => "previous"
    }
  }

  # 添加时间戳
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }

  # 处理 Docker 容器日志
  if [docker] {
    mutate {
      add_field => {
        "container_id" => "%{[docker][container][id]}"
        "container_name" => "%{[docker][container][name]}"
        "container_image" => "%{[docker][container][image]}"
      }
    }
  }

  # 处理系统指标
  if [type] == "system" {
    mutate {
      add_field => {
        "metric_type" => "system"
      }
    }
  }

  # 处理网络数据
  if [type] == "network" {
    mutate {
      add_field => {
        "network_type" => "tcp"
      }
    }
  }

  # 移除敏感字段
  mutate {
    remove_field => ["password", "token", "secret"]
  }
}

output {
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    user => "${ELASTICSEARCH_USERNAME}"
    password => "${ELASTICSEARCH_PASSWORD}"
    ssl_enabled => true
    ssl_certificate_verification => true
    ssl_ca => "/usr/share/logstash/certs/ca.crt"
    index => "logstash-%{+YYYY.MM.dd}"
    document_type => "%{[type]}"
    action => "index"
    workers => 2
    bulk_size => 1000
    retry_on_conflict => 3
  }

  # 错误日志输出
  if "_grokparsefailure" in [tags] {
    file {
      path => "/var/log/logstash/error-%{+YYYY-MM-dd}.log"
      codec => json
    }
  }
} 