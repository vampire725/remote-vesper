# ===========================================
# Logstash 主管道配置文件
# 文件名: main.conf
# 功能: 数据处理管道定义
# 流程: Kafka -> Filter -> Elasticsearch
# ===========================================

# ===========================================
# 输入配置 - 从 Kafka 读取数据
# ===========================================
input {
  kafka {
    # Kafka 集群地址
    bootstrap_servers => "${KAFKA_HOSTS}"
    
    # 订阅的主题列表
    topics => ["${KAFKA_TOPIC}"]
    
    # 客户端标识符
    client_id => "logstash"
    
    # 消费者组标识符
    group_id => "logstash"
    
    # 偏移量重置策略：从最新消息开始消费
    auto_offset_reset => "latest"
    
    # 消费者线程数：并行处理提高性能
    consumer_threads => 2
    
    # 装饰事件：添加 Kafka 元数据信息
    decorate_events => true
    security_protocol => "PLAINTEXT"
#
#    # SASL 认证机制
#    sasl_mechanism => "PLAIN"
#
#    # 安全协议：SASL + SSL
#    security_protocol => "SASL_SSL"
#
#    # SASL 认证配置
#    sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"${KAFKA_USERNAME}\" password=\"${KAFKA_PASSWORD}\";"
  }
}

# ===========================================
# 过滤器配置 - 数据处理和转换
# ===========================================
filter {
  # ----------------------------------------
  # JSON 格式日志解析
  # ----------------------------------------
  if [type] == "json" {
    json {
      # 从 message 字段解析 JSON
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }

    mutate {
      # 将 parsed 下的所有字段提升到顶层
      rename => { "[parsed][%{key}]" => "%{key}" for_each => [parsed] }
      # 删除中间字段
      remove_field => ["parsed", "event", "message"]
    }
  }

  # ----------------------------------------
  # 应用程序日志处理
  # ----------------------------------------
  if [fields][log_type] == "application" {
    grok {
      # 解析标准应用程序日志格式：时间戳 [日志级别] 日志内容
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{GREEDYDATA:log_message}" }
    }
  }

  # ----------------------------------------
  # Nginx 访问日志处理
  # ----------------------------------------
  if [fields][log_type] == "nginx" {
    grok {
      # 解析 Nginx Combined 日志格式
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  }

  # ----------------------------------------
  # 时间戳处理
  # ----------------------------------------
  date {
    # 将解析出的时间戳字段转换为标准时间格式
    match => [ "timestamp", "ISO8601" ]
    # 设置为事件的主时间戳
    target => "@timestamp"
  }

  # ----------------------------------------
  # Docker 容器信息处理
  # ----------------------------------------
  if [docker] {
    mutate {
      # 提取并添加容器相关字段
      add_field => {
        "container_id" => "%{[docker][container][id]}"
        "container_name" => "%{[docker][container][name]}"
        "container_image" => "%{[docker][container][image]}"
      }
    }
  }

  # ----------------------------------------
  # 系统指标数据处理
  # ----------------------------------------
  if [type] == "system" {
    mutate {
      # 标记为系统指标类型
      add_field => {
        "metric_type" => "system"
      }
    }
  }

  # ----------------------------------------
  # 网络数据处理
  # ----------------------------------------
  if [type] == "network" {
    mutate {
      # 标记网络类型
      add_field => {
        "network_type" => "tcp"
      }
    }
  }

  # ----------------------------------------
  # 安全处理：移除敏感信息
  # ----------------------------------------
  mutate {
    # 移除可能包含敏感信息的字段
    remove_field => ["password", "token", "secret"]
  }
}

# ===========================================
# 输出配置 - 发送数据到目标系统
# ===========================================
output {
  # ----------------------------------------
  # Elasticsearch 输出
  # ----------------------------------------
  elasticsearch {
    # Elasticsearch 集群地址
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    
#    # 认证信息
#    user => "${ELASTICSEARCH_USERNAME}"
#    password => "${ELASTICSEARCH_PASSWORD}"
#
#    # SSL/TLS 配置
#    ssl_enabled => true                    # 启用 SSL
#    ssl_certificate_verification => true   # 验证证书
#    ssl_ca => "/usr/share/logstash/certs/ca.crt"  # CA 证书路径
    
    # 索引配置
    index => "logstash-%{+YYYY.MM.dd}"     # 按日期分割索引
    document_type => "%{[type]}"           # 文档类型
    action => "index"                      # 操作类型：索引
    
    # 性能配置
#    workers => 2                          # 工作线程数
    
    # 模板配置
    template => "/usr/share/logstash/templates/logs-template.json"
    template_name => "logstash"
    template_overwrite => true
    document_id => "%{traceID}-%{spanID}"
  }

  # ----------------------------------------
  # 调试输出（开发环境使用）
  # ----------------------------------------
  # 注意：生产环境建议删除或注释此部分
#  if "_grokparsefailure" in [tags] {
#    stdout {
#      codec => rubydebug {
#        # 包含元数据信息用于调试
#        metadata => true
#      }
#    }
#  }
}